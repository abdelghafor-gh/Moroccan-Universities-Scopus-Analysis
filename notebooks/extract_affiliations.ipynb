{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/raw/scopus/2023.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract IDs between parentheses\n",
    "def extract_ids(text):\n",
    "    return re.findall(r'\\((\\d+)\\)', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_country_name(affiliation_full_name):\n",
    "    try:\n",
    "        parts = affiliation_full_name.lower().split(',')\n",
    "        country = parts[-1].strip().lower()\n",
    "        return country\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cities dictionary from the JSON file\n",
    "with open('mappers/cities_mapping.json', 'r', encoding='utf-8') as json_file:\n",
    "    cities_mapping = json.load(json_file)\n",
    "# cities_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_city_name(affiliation_full_name):\n",
    "    \n",
    "    # for each key in the dict, if the key exists return the value\n",
    "    affiliation = affiliation_full_name.lower()\n",
    "    parts = affiliation.split(',')\n",
    "    \n",
    "    expected_city = None\n",
    "    if len(parts) > 1:\n",
    "        expected_city = parts[-2].strip().lower()\n",
    "\n",
    "        for key, value in cities_mapping.items():\n",
    "            if key == expected_city:\n",
    "                return value\n",
    "    \n",
    "    for key, value in cities_mapping.items():\n",
    "        if key in affiliation:\n",
    "            return value\n",
    "    \n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the affiliation by city dictionary from the JSON file\n",
    "with open('mappers/affiliations_by_city.json', 'r', encoding='utf-8') as json_file:\n",
    "    affiliations_by_city = json.load(json_file)\n",
    "\n",
    "# Load the universities by city dictionary from the JSON file\n",
    "with open('mappers/universities_by_city.json', 'r', encoding='utf-8') as json_file:\n",
    "    universities_by_city = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_author_id_name(auth_id_name):\n",
    "    # extract author name\n",
    "    name_pattern = r\"[A-Za-záéíóúÁÉÍÓÚñÑ]+,\\s[A-Za-záéíóúÁÉÍÓÚñÑ]+\"\n",
    "    name_match = re.search(name_pattern, auth_id_name)\n",
    "\n",
    "    # extract id\n",
    "    id_match = re.search(r'\\((\\d+)\\)', auth_id_name)\n",
    "    \n",
    "    if id_match and name_match:\n",
    "        author_id = id_match.group(1)\n",
    "        author_name = name_match.group()\n",
    "        return author_id, author_name\n",
    "    else:\n",
    "        return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_accents(text):\n",
    "    # Normalize the text to decompose characters with accents\n",
    "    normalized_text = unicodedata.normalize('NFKD', text)\n",
    "    # Filter out the accents and keep only ASCII characters\n",
    "    return ''.join(char for char in normalized_text if not unicodedata.combining(char))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_digits(affiliation):\n",
    "    # Patterns for variations\n",
    "    patterns = {\n",
    "        r'\\b(first|1st|1er|i)\\b': '1',  # Variations of 1\n",
    "        r'\\b(second|2nd|ii)\\b': '2',    # Variations of 2\n",
    "        r'\\b(fifth|5th|v)\\b': '5'       # Variations of 5\n",
    "    }\n",
    "    \n",
    "    normalized_affiliation = affiliation\n",
    "    for pattern, replacement in patterns.items():\n",
    "        # Replace matched patterns with their respective replacements\n",
    "        normalized_affiliation = re.sub(pattern, replacement, normalized_affiliation)\n",
    "    return normalized_affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_affiliation(affiliation):\n",
    "    # lower case affiliation name\n",
    "    affiliation = affiliation.lower()\n",
    "    # remove accents to simply the mapping\n",
    "    affiliation = remove_accents(affiliation)\n",
    "    # normalize digits\n",
    "    affiliation = normalize_digits(affiliation)\n",
    "    \n",
    "    return affiliation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_affiliation_name(affiliation_full_name, city):\n",
    "    \n",
    "    if not city:\n",
    "        return None, None\n",
    "    \n",
    "    affiliation = normalize_affiliation(affiliation_full_name)\n",
    "\n",
    "    for key, values in affiliations_by_city[city].items():\n",
    "        for val in values:\n",
    "            if re.search(rf'\\b{re.escape(val)}\\b', affiliation):\n",
    "                return key, val\n",
    "\n",
    "    if city not in universities_by_city.keys():\n",
    "        return None, None\n",
    "        \n",
    "    for key, values in universities_by_city[city].items():\n",
    "        for val in values:\n",
    "            if re.search(rf'\\b{re.escape(val)}\\b', affiliation):\n",
    "                return key, val\n",
    "    \n",
    "    return None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def duplicate_rows_by_author(df):\n",
    "    new_df = pd.DataFrame(columns=[\"author_id\", \"author_name\", \"affiliation_full_name\", \"city\", \"affiliation\", \"affiliation_id\"])\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        authors_with_ids = row[\"Author full names\"].split(';')\n",
    "        authors_with_affiliations = row[\"Affiliations\"].split(';')\n",
    "        ids, author_names, affiliations_full_name, cities, affiliations, affiliation_ids = [], [], [], [], [], []\n",
    "        \n",
    "        for auth_id_name, aff in zip(authors_with_ids, authors_with_affiliations):\n",
    "            \n",
    "            author_id, author_name = extract_author_id_name(auth_id_name)\n",
    "            \n",
    "            if not (author_id and author_name):\n",
    "                continue\n",
    "\n",
    "            # Extract country\n",
    "            country = extract_country_name(aff)\n",
    "            if country not in [\"morocco\", \"maroc\"]:\n",
    "                continue\n",
    "            \n",
    "            # Extract city\n",
    "            city = extract_city_name(aff)\n",
    "\n",
    "            # Extract affiliation and aff id\n",
    "            aff_id, affiliation = extract_affiliation_name(aff, city)\n",
    "\n",
    "            ids.append(author_id)\n",
    "            author_names.append(author_name)\n",
    "            affiliations_full_name.append(aff)\n",
    "            cities.append(city)\n",
    "            affiliation_ids.append(aff_id)\n",
    "            affiliations.append(affiliation)\n",
    "\n",
    "        data = pd.DataFrame({\n",
    "            \"author_id\": ids,\n",
    "            \"author_name\": author_names,\n",
    "            \"affiliation_full_name\": affiliations_full_name,\n",
    "            \"city\": cities,\n",
    "            \"affiliation\": affiliations,\n",
    "            \"affiliation_id\": affiliation_ids\n",
    "        })\n",
    "\n",
    "        new_df = pd.concat([new_df, data], ignore_index=True)\n",
    "        \n",
    "    return new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df = duplicate_rows_by_author(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_df.to_csv(\"../data/transformed/2023.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "author_id                28948\n",
       "author_name              28948\n",
       "affiliation_full_name    28948\n",
       "city                     26112\n",
       "affiliation              19522\n",
       "affiliation_id           19522\n",
       "dtype: int64"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# trans_df['affiliation']\n",
    "# trans_df.columns\n",
    "trans_df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "transformed = trans_df[['affiliation_full_name', 'affiliation']]\n",
    "transformed.to_csv(\"../data/transformed/result.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
